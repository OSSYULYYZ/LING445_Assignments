\documentclass[12pt, letterpaper]{article} 

\usepackage{amsmath,amsthm,amssymb}
\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{textcomp, listings, hyperref}
\usepackage[variablett]{lmodern}
\usepackage{fancyhdr}
\usepackage{mathpazo}

\lstset{
 basicstyle=\ttfamily,
 columns=fullflexible,
 upquote,
 keepspaces,
 literate={*}{{\char42}}1
 {-}{{\char45}}1
}

\begin{document}

\pagestyle{fancy}
\lhead{LING445\\Assignment 2}
\rhead{ID:260621270\\ZHU, Jake}
\paragraph{Problem 1:}

In these exercises, we are going to be processing some natural
linguistic data, the first paragraph of Moby Dick. We will first write
some procedures that help us to manipulate this corpus. We will then
start analyzing this data using some probabilistic models.

We will start by defining the variable \texttt{moby-word-tokens},
which is a list containing all of the words from our corpus.

\begin{lstlisting}
(def moby-word-tokens '(CALL me Ishmael . Some years ago never mind
     how long precisely having little or no money in my purse , and
     nothing particular to interest me on shore , I thought I would
     sail about a little and see the watery part of the world .  It is
     a way I have of driving off the spleen , and regulating the
     circulation . Whenever I find myself growing grim about the mouth
     whenever it is a damp , drizzly November in my soul whenever I
     find myself involuntarily pausing before coffin warehouses , and
     bringing up the rear of every funeral I meet and especially
     whenever my hypos get such an upper hand of me , that it requires
     a strong moral principle to prevent me from deliberately stepping
     into the street , and methodically knocking people's hats off
     then , I account it high time to get to sea as soon as I can .
     This is my substitute for pistol and ball . With a philosophical
     flourish Cato throws himself upon his sword I quietly take to the
     ship .  There is nothing surprising in this . If they but knew it
     , almost all men in their degree , some time or other , cherish
     very nearly the same feelings toward the ocean with me .))
\end{lstlisting}

Below we have defined the function \texttt{member-of-list}, which
takes two arguments, \texttt{w} and \texttt{l}. The function returns
\texttt{true} if the element \texttt{w} is a member of the list
\texttt{l}, and \texttt{false} otherwise. For example, if \texttt{w}
equals \texttt{'the} and \texttt{l} equals \texttt{(list 'the 'man
  'is)}, then the function will return \texttt{true}. In contrast, if
\texttt{w} equals \texttt{'the} and \texttt{l} equals \texttt{(list
  'man 'is)}, then the function will return false.

\begin{lstlisting}
(defn member-of-list? [w l]
  (if (empty? l)
    false
    (if (= w (first l))
      true
      (member-of-list? w (rest l)))))
\end{lstlisting}

Below we have defined the skeleton for the function
\texttt{get-vocabulary}. This function takes two arguments,
\texttt{word-tokens} and \texttt{vocab}. \texttt{word-tokens} is a
list of words, and the function should return the list of unique words
occuring in word-tokens. This list of unique words is called a
vocabulary. For example, if word-tokens is equal to \texttt{(list 'the
  'man 'is 'man 'the)}, then get-vocabulary should return (list 'the
'man 'is).

Fill in the missing parts of this function. When you call
\texttt{(get-vocabulary moby-word-tokens '())}, you will get back a
list of all of the unique words occuring in moby-word-tokens. Give
this the name \texttt{moby-vocab}.

\begin{lstlisting}
;;(defn get-vocabulary [word-tokens vocab]
;;  (if (empty? word-tokens)
;;    vocab
;;    (if (member-of-list? ;;finish this line
;;      (get-vocabulary ;;finish this line
;;      (get-vocabulary ;;finish this line
\end{lstlisting}

\paragraph{Answer 1:}~\begin{lstlisting}
(defn get-vocabulary [word-tokens vocab]
  (if (empty? word-tokens)
    vocab
    (if (member-of-list? (first word-tokens) vocab)
      (get-vocabulary (rest word-tokens) vocab)
      (get-vocabulary (rest word-tokens)
        (cons (first word-tokens) vocab)))))
(def moby-vocab (get-vocabulary moby-word-tokens '()))
\end{lstlisting}

\hrulefill
\paragraph{Problem 2:}

Define a function \texttt{get-count-of-word}. This function should
take three arguments, \texttt{w}, \texttt{word-tokens}, and
\texttt{count}. \texttt{w} is a word and \texttt{word-tokens} is a
list of words. When you call \texttt{(get-count-of-word w word-tokens
  0)}, the function should return the number of occurrences of the
word \texttt{w} in the list \texttt{word-tokens}. For example, if
word-tokens equals \texttt{(list 'the 'the 'man)}, and \texttt{w}
equals \texttt{'the}, then the function should return \texttt{2}. If
\texttt{word-tokens} is the same, but \texttt{w} equals \texttt{'man},
then the function should return \texttt{1}.

\begin{lstlisting}
;;(defn get-count-of-word [w word-tokens count]
  ;;fill this in
\end{lstlisting}

Below we have defined the function \texttt{get-word-counts}, which
takes two arguments, \texttt{vocab} and
\texttt{word-tokens}. \texttt{vocab} is assumed to be a list of the
unique words that occur in the list \texttt{word-tokens}. The function
returns the number of times each word in vocab occurs in
word-tokens. For example, suppose \texttt{vocab} equals \texttt{(list
  'man 'the 'is)}, and \texttt{word-tokens} equals \texttt{(list 'the
  'man 'is 'is)}. Then the function would return \texttt{(list 1 1
  2)}, corresponding to the number of times \texttt{'man},
\texttt{'the}, and \texttt{'is} occur in \texttt{word-tokens}.

\begin{lstlisting}
(defn get-word-counts [vocab word-tokens]
  (let [count-word 
    (fn [w] (get-count-of-word w word-tokens 0))]
    (map count-word vocab)))
\end{lstlisting}

\paragraph{Answer 2:}~\begin{lstlisting}
(defn get-count-of-word [w word-tokens count]
  (if (empty? word-tokens)
    count
    (if (= w (first word-tokens))
      (get-count-of-word w (rest word-tokens) (+ count 1))
      (get-count-of-word w (rest word-tokens) count))))
\end{lstlisting}

\hrulefill
\paragraph{Problem 3:}

Use the function \texttt{get-word-counts}, and the other variables we have
defined, to define a variable \texttt{moby-word-frequencies}. This variable
should contain the number of times each word in \texttt{moby-vocab} occurs in
\texttt{moby-word-tokens}.

In class we defined the functions \texttt{normalize}, \texttt{flip},
and \texttt{sample-categorical}. These functions are very useful for
us, and are included below.

\begin{lstlisting}
(defn flip [p]
  (if (< (rand 1) p)
    true
    false))

(defn normalize [params]
  (let [sum (apply + params)]
    (map (fn [x] (/ x sum)) params)))

(defn sample-categorical [outcomes params]
  (if (flip (first params))
    (first outcomes)
    (sample-categorical (rest outcomes) 
      (normalize (rest params)))))
\end{lstlisting}

Let's define a function that returns a particular probability
distribution, the uniform distribution. The uniform distribution is
the distribution which assigns equal probability to every possible
outcome.
  
The function \texttt{uniform-distribution} takes a single argument,
\texttt{outcomes}, which is a list of length n. The function should
return a list containing the number \texttt{1/n} repeated n times. For
example, if outcomes equals \texttt{(list 'the 'a 'every)}, then this
function will return \texttt{(list 1/3 1/3 1/3)}. This list can be
interpreted as a probability distribution over the outcomes, which
assigns equal probability to each of them.

\begin{lstlisting}
(defn create-uniform-distribution [outcomes]
  (let [num-outcomes (count outcomes)]
    (map (fn [x] (/ 1 num-outcomes))
      outcomes)))
\end{lstlisting}


\paragraph{Answer 3:}~\begin{lstlisting}
(def moby-word-frequencies (get-word-counts moby-vocab moby-word-tokens))
\end{lstlisting}

\hrulefill
\paragraph{Problem 4:}

Using \texttt{create-uniform-distribution} and
\texttt{sample-categorical}, write a function
\texttt{sample-uniform-BOW-sentence} that takes a number \texttt{n}
and a list \texttt{vocab}, and returns a sentence of length
\texttt{n}. Each word in the sentence should be generated
independently from the uniform distribution over vocab. For example,
given \texttt{n} equal to \texttt{4} and \texttt{vocab} equal to
\texttt{(list 'the 'a 'every)}, a possible return value for this
function is \texttt{(list 'a 'the 'the 'a)}.

Note that this is a bag of words model, as defined in class. That is,
we assume every element of the list is generated independently. We
will call this the uniform bag of words model.

\paragraph{Answer 4:} \url{https://foundations-computational-linguistics.github.io/chapters/11-BagOfWords.html}
\begin{lstlisting}
(defn list-unfold [generator n]
  (if (= n 0)
    '()
    (cons (generator)
      (list-unfold generator (- n 1)))))
(defn sample-uniform-BOW-sentence [n vocab]
  (list-unfold 
    (fn [] (sample-categorical vocab
      (create-uniform-distribution vocab))) n))
\end{lstlisting}

\hrulefill
\paragraph{Problem 5:}

Define a function \texttt{compute-uniform-BOW-prob}, which takes two
arguments, \texttt{vocab} and \texttt{sentence}. \texttt{vocab} is the
list of all words in the vocabulary, and sentence is a list of
observed words. The function should return the probability of the
sentence according to the uniform bag of words model.

For example, if \texttt{vocab} equals \texttt{(list 'the 'a 'every)},
and sentence equals \texttt{(list 'every 'every)}, then the function
should return $\frac{1}{9}$.

\paragraph{Answer 5:}
\url{https://rosettacode.org/wiki/Exponentiation_operator}
\begin{lstlisting}
(defn ** [x n] (reduce * (repeat n x)))
(defn compute-uniform-BOW-prob [vocab sentence]
  (** (first (create-uniform-distribution vocab))
    (count sentence)))
\end{lstlisting}

\hrulefill
\paragraph{Problem 6:}

Using \texttt{sample-uniform-BOW-sentence} and \texttt{moby-vocab},
sample a 3-word sentence from the vocabulary of our Moby Dick
corpus. This will be a sample from the uniform bag of words model for
this vocabulary. Repeat this process a number of times. For each of
these 3-word sentences, use \texttt{compute-uniform-BOW-prob} to
compute the probability of the sentence according to the uniform bag
of words model. What do you notice? Why is this true?

\paragraph{Answer 6:}~\begin{lstlisting}
(def moby-sentence (sample-uniform-BOW-sentence 3 moby-vocab))
(compute-uniform-BOW-prob moby-vocab moby-sentence)
(warehouses deliberately myself)
1/2744000
(flourish purse strong)
1/2744000
(part upper off)
1/2744000
\end{lstlisting}

The probability of the sentence is always $\frac{1}{2744000}$. This is because we are using a uniform Bag of Words Model, and each word in the vocab has the same likelihood of appearing.

\hrulefill
\paragraph{Problem 7:}

In class we looked at a more general version of the bag of words
model, in which different words in the vocabulary can be assigned
different probabilities. We defined a function \texttt{sample-BOW-sentence},
which returns a sentence sampled from the bag of words model that we
have specified. Below we have included a slight variant of the
function which we defined in class. Previously the variables
vocabulary and probabilities were defined outside of the function. In
the current version, they are passed in as arguments. The function is
identical otherwise.

\begin{lstlisting}
(defn sample-BOW-sentence [len vocabulary probabilities]
  (if (= len 0)
    '()
    (cons (sample-categorical vocabulary probabilities)
    (sample-BOW-sentence (- len 1) vocabulary probabilities))))
\end{lstlisting}

The function \texttt{sample-BOW-sentence} allows us to sample a
sentence given arbitrary probabilities for the words in our
vocabulary. Let's make use of this power and define a distribution
over the vocabulary which is better than the uniform distribution. We
will use the word frequencies for our Moby Dick corpus to
\emph{estimate} a better distribution.
  
Above we defined the variable \texttt{moby-word-frequencies}, which
contains the frequency of every word that occurs in our Moby Dick
corpus. Using \texttt{normalize} and \texttt{moby-word- frequencies},
define a variable \texttt{moby-word-probabilities}. This variable
should contain probabilities for every word in \texttt{moby-vocab}, in
proportion to its frequency in the text. A word which occurs 2 times
should receive twice as much probability as a word which occurs 1
time.

\paragraph{Answer 7:}~\begin{lstlisting}
(def moby-word-probabilities (normalize moby-word-frequencies))
\end{lstlisting}

\hrulefill
\paragraph{Problem 8:}

Using \texttt{sample-BOW-sentence}, sample a 3-word sentence from a
bag of words model, in which the probabilities are set to be those in
\texttt{moby-word-probabilities}. Repeat this process a number of
times, and write down the sentences that you collect through this
process.

\paragraph{Answer 8:}~\begin{lstlisting}
(sample-BOW-sentence 3 moby-vocab moby-word-probabilities)
(CALL degree whenever)
(to with account)
(interest other nothing)
\end{lstlisting}

\hrulefill
\paragraph{Problem 9:}

Define a function \texttt{lookup-probability}, which takes three
arguments, \texttt{w}, \texttt{outcomes}, and
\texttt{probs}. \texttt{probs} represents a probability distribution
over the elements of \texttt{outcomes}. For example, if outcomes is
\texttt{(list 'the 'a 'every)}, then \texttt{probs} may be equal to
\texttt{(list 0.2 0.5 0.3)}. The first number in \texttt{probs} is the
probability of the first element of outcomes, the second number in
probs is the probability of the second element of outcomes, and so on.

\texttt{lookup-probability} should look up the probability of the
element \texttt{w}. For example, if \texttt{w} equals \texttt{'the},
then look-up probability should return \texttt{0.2}. If \texttt{w}
equals \texttt{'a}, then \texttt{lookup- probability} should return
\texttt{0.5}.

Below we have defined the function \texttt{product}. This function takes a list
of numbers as its argument, and returns the product of these numbers.

\begin{lstlisting}
(defn product [l]
  (apply * l))
\end{lstlisting}

\paragraph{Answer 9:}~\begin{lstlisting}
(defn lookup-probability [w outcomes probs]
  (if (= (first outcomes) w)
   (first probs)
   (lookup-probability w (rest outcomes) (rest probs))))
\end{lstlisting}

\hrulefill
\paragraph{Problem 10:}

Using \texttt{lookup-probability} and \texttt{product}, define a
function \texttt{compute-BOW- prob} which takes three arguments,
\texttt{sentence}, \texttt{vocabulary}, and
\texttt{probabilities}. The arguments \texttt{vocabulary} and
\texttt{probabilities} are used to define a bag of words model with
the associated probability distribution over vocabulary words. The
function should compute the probability of the sentence (which is a
list of words) according to the bag of words model.

This function is a generalization of the function
\texttt{compute-uniform-BOW-prob} that you defined above.

\paragraph{Answer 10:}~\begin{lstlisting}
(defn get-prob [sentence vocabulary probabilities]
  (if (empty? sentence)
    '()
    (cons (lookup-probability (first sentence) vocabulary probabilities)
    (get-prob (rest sentence) vocabulary probabilities))))
(defn compute-BOW-prob [sentence vocabulary probabilities]
  (product (get-prob sentence vocabulary probabilities)))
\end{lstlisting}

\hrulefill
\paragraph{Problem 11:}

In problem 8, you collected a number of 3-word sentences. These
sentences were generated from a bag of words model in which the
probabilities were set to those in \texttt{moby-word-probabilities},
which reflect the relative frequency of the words in the Moby Dick
corpus. Use \texttt{compute-BOW-prob} to compute the probability of
these sentences according to the bag of words model. How does your
answer differ from problem 6?

Choose one of the 3-word sentences that you have generated. Can you
construct a different sentence which has the same probability
according to the bag of words model? When computing the probability of
a sentence under a bag of words model, what information about the
sentence suffices to compute this probability?

\paragraph{Answer 11:}~\begin{lstlisting}
(compute-BOW-prob moby-sentence moby-vocab moby-word-probabilities)
(CALL degree whenever)
3/9129329
(to with account)
5/9129329
(interest other nothing)
2/9129329
\end{lstlisting}
I will use $C(list)$ to denote the count or the size of a list. The probability that \texttt{(to with account)} is the given sentence is $\frac{5}{9129329}$. The sentence \texttt{(grim to way)} also has a probability of $\frac{5}{9129329}$. In q6, we assumed that each word had the same frequency. Therefore the denominator was $(C(vocab))^3 = 140^3 = 2744000$. In q11, we took the frequencies of the words into consideration, so $(C(tokens))^3 = 209^3 = 9129329$. As stated in this paragraph, the frequencies of the words are necessary for the probability of a sentence under a bag of words model.
\end{document}
